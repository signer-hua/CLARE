# -*- coding: utf-8 -*-
"""
å¯¹æŠ—æ”»å‡»è„šæœ¬ - é€‚é…æ¨¡å‹ç‰ˆæœ¬
æ”¯æŒä¸¤ç§æ¨¡å‹ï¼šBERTæ¨¡å‹å’Œåˆ†ç±»å™¨æ¨¡å‹
"""

import warnings
import os
import sys
import torch
import torch.nn as nn
import json
import pandas as pd
import re
from transformers import AutoTokenizer, AutoModel
import copy
import numpy as np
from tqdm import tqdm
import time
import pickle

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.simplefilter(action='ignore', category=FutureWarning)

# åœç”¨è¯åˆ—è¡¨ï¼ˆé’ˆå¯¹ä¸­æ–‡å¯¹è¯ä¼˜åŒ–ï¼‰
filter_words = ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°',
                'è¯´', 'è¦', 'å»', 'ä½ ',
                'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'è¿™ä¸ª',
                'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ',
                'ä¸ºä»€ä¹ˆ', 'å—', 'å‘¢', 'å§', 'å•Š', 'å‘€', 'å“¦', 'å—¯', 'å‘ƒ', 'ç„¶å', 'ä½†æ˜¯', 'å¯æ˜¯', 'ä¸è¿‡', 'è€Œä¸”',
                'æ‰€ä»¥', 'å› ä¸º', 'å¦‚æœ',
                'è™½ç„¶', 'å³ä½¿', 'æ—¢ç„¶', 'ä¸ºäº†', 'å…³äº', 'å¯¹äº', 'æ ¹æ®', 'æŒ‰ç…§', 'é€šè¿‡', 'éšç€', 'ä½œä¸º', 'è€Œä¸”', 'æˆ–è€…',
                'è¿˜æ˜¯', 'ä¸ä»…', 'è€Œä¸”',
                'æ—¢', 'åˆ', 'æ— è®º', 'ä¸ç®¡', 'å°½ç®¡', 'å³ä½¿', 'å‡å¦‚', 'å€˜è‹¥', 'åªè¦', 'åªæœ‰', 'é™¤é', 'æ— è®º', 'ä¸è®º',
                'ä¸ç®¡', 'å°½ç®¡', 'å³ä½¿',
                'æ—¢ç„¶', 'å› ä¸º', 'æ‰€ä»¥', 'å› æ­¤', 'äºæ˜¯', 'ç„¶å', 'æ¥ç€', 'æœ€å', 'é¦–å…ˆ', 'å…¶æ¬¡', 'å†æ¬¡', 'å¦å¤–', 'æ­¤å¤–',
                'åŒæ—¶', 'åŒæ ·',
                'ç›¸å', 'åè€Œ', 'ç„¶è€Œ', 'å¯æ˜¯', 'ä½†æ˜¯', 'ä¸è¿‡', 'åªæ˜¯', 'å´æ˜¯', 'å€’æ˜¯', 'å°±æ˜¯', 'éƒ½æ˜¯', 'æ€»æ˜¯', 'åˆæ˜¯',
                'è¿˜æ˜¯', 'ä¹Ÿæ˜¯',
                'å°±æ˜¯', 'å°±æ˜¯', 'å°±æ˜¯', 'å°±æ˜¯', 'å°±æ˜¯', 'å°±æ˜¯', 'å°±æ˜¯', 'å°±æ˜¯', 'å°±æ˜¯', 'å°±æ˜¯']

filter_words = set(filter_words)


class FraudDialogueFeature:
    """
    æ¬ºè¯ˆå¯¹è¯ç‰¹å¾ç±»ï¼Œæ‰©å±•åŸå§‹Featureç±»
    """

    def __init__(self, seq_a, label, original_info=None):
        self.label = label
        self.seq = seq_a
        self.final_adverse = seq_a
        self.query = 0
        self.change = 0
        self.success = 0  # 0:å¤±è´¥, 1:æ›¿æ¢è¿‡å¤š, 2:æœªæ‰¾åˆ°å¯¹æŠ—æ ·æœ¬, 3:åŸå§‹é”™è¯¯, 4:æ”»å‡»æˆåŠŸ
        self.sim = 0.0
        self.changes = []
        self.original_info = original_info  # ä¿å­˜åŸå§‹ä¿¡æ¯
        self.attack_type = "word_replacement"  # æ”»å‡»ç±»å‹


class BertClassifier(nn.Module):
    def __init__(self, bert_model_name='bert-base-chinese', num_classes=2):
        super(BertClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(bert_model_name)
        hidden_size = self.bert.config.hidden_size
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_classes)

    def forward(self, input_ids, attention_mask=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits


class Classifier(nn.Module):
    """åˆ†ç±»å™¨"""

    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_classes=2):
        super(Classifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, input_ids, attention_mask=None):
        # åµŒå…¥å±‚
        embedded = self.embedding(input_ids)

        # LSTM
        lstm_out, _ = self.lstm(embedded)

        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_hidden = lstm_out[:, -1, :]

        # åˆ†ç±»
        output = self.dropout(last_hidden)
        logits = self.fc(output)

        return logits


class Tokenizer:
    """å­—ç¬¦çº§tokenizer"""

    def __init__(self, vocab_size=5000):
        self.vocab_size = vocab_size
        self.char_to_idx = {}
        self.idx_to_char = {}
        self.unk_token = '[UNK]'
        self.pad_token = '[PAD]'
        self.cls_token = '[CLS]'
        self.sep_token = '[SEP]'

    def encode(self, text, max_length=128):
        """ç¼–ç æ–‡æœ¬"""
        tokens = [self.char_to_idx.get(char, self.char_to_idx[self.unk_token]) for char in text[:max_length - 2]]

        # æ·»åŠ ç‰¹æ®Štoken
        tokens = [self.char_to_idx[self.cls_token]] + tokens + [self.char_to_idx[self.sep_token]]

        # å¡«å……
        if len(tokens) < max_length:
            tokens = tokens + [self.char_to_idx[self.pad_token]] * (max_length - len(tokens))
        else:
            tokens = tokens[:max_length]
            tokens[-1] = self.char_to_idx[self.sep_token]

        # åˆ›å»ºattention mask
        attention_mask = [1 if token != self.char_to_idx[self.pad_token] else 0 for token in tokens]

        return {
            'input_ids': torch.tensor(tokens, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)
        }


def clean_dialogue_for_attack(text):
    """
    æ”»å‡»å‰çš„å¯¹è¯æ¸…æ´—
    """
    if not isinstance(text, str):
        return ""

    # ç§»é™¤ç‰¹æ®Šæ ‡è®°å’Œå¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()

    return text


def get_fraud_data_cls(data_path):
    """
    åŠ è½½æ¬ºè¯ˆå¯¹è¯æ•°æ®
    """
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")

    texts = []
    labels = []

    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # è·³è¿‡è¡¨å¤´
        if lines[0].strip() == 'text_a\tlabel':
            lines = lines[1:]

        for line in lines:
            line = line.strip()
            if not line:
                continue
            parts = line.split('\t')
            if len(parts) >= 2:
                text = clean_dialogue_for_attack(parts[0])
                if text:
                    texts.append(text)
                    try:
                        labels.append(int(parts[1]))
                    except:
                        labels.append(1)

        print(f"âœ… åŠ è½½äº† {len(texts)} æ¡æ ·æœ¬")

    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")

    return list(zip(texts, labels))


def _tokenize_chinese_dialogue(seq, tokenizer):
    """
    ä¸­æ–‡å¯¹è¯åˆ†è¯å¤„ç†
    """
    seq = seq.replace('\n', '').replace('\t', ' ')

    # å­—ç¬¦çº§åˆ†è¯ï¼ˆå¯¹äºä¸­æ–‡æ›´åˆé€‚ï¼‰
    words = list(seq)

    sub_words = []
    keys = []
    index = 0

    for word in words:
        # å¯¹äºTokenizerï¼Œç›´æ¥è¿”å›å­—ç¬¦
        if isinstance(tokenizer, Tokenizer):
            sub = [word]
        else:
            # BERT tokenizer
            sub = tokenizer.tokenize(word)
        sub_words += sub
        keys.append([index, index + len(sub)])
        index += len(sub)

    return words, sub_words, keys


def get_important_scores_fraud(words, tgt_model, orig_prob, orig_label, orig_probs, tokenizer, batch_size, max_length,
                               device):
    """
    è®¡ç®—è¯é‡è¦æ€§åˆ†æ•°
    """
    masked_words = []

    # åˆ›å»ºæ©ç ç‰ˆæœ¬
    for i in range(len(words)):
        masked = words.copy()
        masked[i] = '[UNK]' if not isinstance(tokenizer, Tokenizer) else tokenizer.unk_token
        masked_words.append(masked)

    # å‡†å¤‡è¾“å…¥
    texts = [''.join(words) for words in masked_words]

    all_input_ids = []
    all_attention_masks = []

    for text in texts:
        if isinstance(tokenizer, Tokenizer):
            inputs = tokenizer.encode(text, max_length)
            all_input_ids.append(inputs['input_ids'].unsqueeze(0))
            all_attention_masks.append(inputs['attention_mask'].unsqueeze(0))
        else:
            inputs = tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=max_length,
                return_tensors='pt'
            )
            all_input_ids.append(inputs['input_ids'])
            all_attention_masks.append(inputs['attention_mask'])

    seqs = torch.cat(all_input_ids, dim=0).to(device)
    masks = torch.cat(all_attention_masks, dim=0).to(device)

    # æ‰¹é‡è®¡ç®—æ¦‚ç‡
    leave_1_probs = []
    for i in range(0, len(seqs), batch_size):
        batch_seqs = seqs[i:i + batch_size]
        batch_masks = masks[i:i + batch_size]

        with torch.no_grad():
            outputs = tgt_model(batch_seqs, batch_masks)
            leave_1_prob_batch = torch.softmax(outputs, -1)
            leave_1_probs.append(leave_1_prob_batch)

    if leave_1_probs:
        leave_1_probs = torch.cat(leave_1_probs, dim=0)
    else:
        leave_1_probs = torch.zeros(len(words), orig_probs.size(-1)).to(device)

    leave_1_probs_argmax = torch.argmax(leave_1_probs, dim=-1)

    # è®¡ç®—é‡è¦æ€§åˆ†æ•°
    import_scores = (
            orig_prob
            - leave_1_probs[:, orig_label]
            + (leave_1_probs_argmax != orig_label).float()
            * (leave_1_probs.max(dim=-1)[0] - torch.index_select(orig_probs, 0, leave_1_probs_argmax))
    ).data.cpu().numpy()

    return import_scores


def _predict_with_target_model(text, tgt_model, tokenizer, device, max_length):
    if isinstance(tokenizer, Tokenizer):
        inputs = tokenizer.encode(text, max_length)
        input_ids = inputs['input_ids'].unsqueeze(0).to(device)
        attention_mask = inputs['attention_mask'].unsqueeze(0).to(device)
    else:
        inputs = tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=max_length,
            return_tensors='pt'
        )
        input_ids = inputs['input_ids'].to(device)
        attention_mask = inputs['attention_mask'].to(device)

    with torch.no_grad():
        outputs = tgt_model(input_ids, attention_mask)
        probs = torch.softmax(outputs, -1).squeeze()

    return probs


def _is_valid_mlm_token(token, tokenizer):
    if token is None:
        return False
    token = str(token)
    if token in {tokenizer.unk_token, tokenizer.pad_token, tokenizer.cls_token, tokenizer.sep_token, tokenizer.mask_token}:
        return False
    if token.startswith('##'):
        return False
    if token.startswith('[') and token.endswith(']'):
        return False
    if token.strip() == '':
        return False
    return True


def _mlm_candidates_for_single_mask(text_with_mask, tokenizer, mlm_model, device, k, max_length):
    if mlm_model is None:
        return []

    inputs = tokenizer(
        text_with_mask,
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='pt'
    )
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    mask_id = tokenizer.mask_token_id
    mask_positions = (input_ids[0] == mask_id).nonzero(as_tuple=False)
    if mask_positions.numel() == 0:
        return []

    mask_index = int(mask_positions[0].item())

    with torch.no_grad():
        outputs = mlm_model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits[0, mask_index, :]
        probs = torch.softmax(logits, -1)

    top_probs, top_ids = torch.topk(probs, k)
    tokens = tokenizer.convert_ids_to_tokens(top_ids.tolist())
    return [(tok, float(score)) for tok, score in zip(tokens, top_probs.tolist())]


def _generate_candidates_clare(text, idx, tokenizer, mlm_model, k, max_length):
    words = list(text)
    if idx < 0 or idx >= len(words):
        return []

    original_char = words[idx]
    masked_text = ''.join(words[:idx]) + tokenizer.mask_token + ''.join(words[idx + 1:])
    raw = _mlm_candidates_for_single_mask(masked_text, tokenizer, mlm_model, device=next(mlm_model.parameters()).device,
                                          k=k, max_length=max_length)
    candidates = []
    for tok, score in raw:
        if not _is_valid_mlm_token(tok, tokenizer):
            continue
        if tok == original_char:
            continue
        if tok in filter_words:
            continue
        if len(tok) != 1:
            continue
        candidates.append((tok, score))
    return candidates


def _generate_insert_candidates_clare(text, idx, tokenizer, mlm_model, k, max_length):
    words = list(text)
    if idx < 0 or idx > len(words):
        return []

    masked_text = ''.join(words[:idx]) + tokenizer.mask_token + ''.join(words[idx:])
    raw = _mlm_candidates_for_single_mask(masked_text, tokenizer, mlm_model, device=next(mlm_model.parameters()).device,
                                          k=k, max_length=max_length)
    candidates = []
    for tok, score in raw:
        if not _is_valid_mlm_token(tok, tokenizer):
            continue
        if tok in filter_words:
            continue
        if len(tok) != 1:
            continue
        candidates.append((tok, score))
    return candidates


def attack_fraud_dialogue(feature, tgt_model, mlm_model, tokenizer, k, batch_size, device,
                          max_length=512, use_bpe=0, threshold_pred_score=0.3, max_change_rate=0.4):
    """
    é’ˆå¯¹æ¬ºè¯ˆå¯¹è¯çš„æ”»å‡»å‡½æ•°
    """
    feature.attack_type = "clare"

    original_text = feature.seq
    current_text = original_text

    if not isinstance(current_text, str) or current_text.strip() == '':
        feature.success = 2
        return feature

    orig_probs = _predict_with_target_model(current_text, tgt_model, tokenizer, device, max_length)
    feature.query += 1
    orig_label = int(torch.argmax(orig_probs).item())
    current_label = orig_label

    if orig_label != feature.label:
        feature.success = 3
        return feature

    max_changes = int(max_change_rate * max(1, len(original_text)))

    similar_chars = {
        'æˆ‘': ['ä½ ', 'ä»–', 'å¥¹'],
        'ä½ ': ['æˆ‘', 'ä»–', 'å¥¹'],
        'ä»–': ['å¥¹', 'å®ƒ', 'ä½ '],
        'å¥¹': ['ä»–', 'å®ƒ', 'ä½ '],
        'æ˜¯': ['å¦', 'é'],
        'ä¸': ['æ²¡', 'å‹¿'],
        'æœ‰': ['æ— ', 'æ²¡'],
        'æ²¡': ['ä¸', 'æ— ', 'æœª'],
        'é’±': ['æ¬¾', 'é‡‘', 'èµ„'],
        'å¡': ['å·', 'è¯', 'è´¦'],
        'è½¬': ['æ±‡', 'æ‰“', 'ä»˜'],
        'è´¦': ['æ¬¾', 'è´¹', 'é‡‘']
    }

    while feature.change < max_changes:
        words = list(current_text)
        if len(words) == 0:
            break

        current_probs = _predict_with_target_model(current_text, tgt_model, tokenizer, device, max_length)
        feature.query += 1
        current_label = int(torch.argmax(current_probs).item())
        if current_label != orig_label:
            feature.final_adverse = current_text
            feature.success = 4
            return feature

        current_orig_prob = float(current_probs[orig_label].item())

        important_scores = get_important_scores_fraud(
            words, tgt_model, current_probs.max(), orig_label, current_probs,
            tokenizer, batch_size, max_length, device
        )
        feature.query += len(words)

        ranked = sorted(enumerate(important_scores), key=lambda x: x[1], reverse=True)
        candidate_positions = [idx for idx, _ in ranked[:min(20, len(ranked))]]

        best = None

        for idx in candidate_positions:
            tgt_char = words[idx]
            if tgt_char in filter_words:
                continue
            if tgt_char.strip() == '':
                continue

            replace_candidates = []
            insert_candidates_before = []

            if isinstance(tokenizer, Tokenizer):
                replace_candidates = [(c, 1.0) for c in similar_chars.get(tgt_char, []) if c != tgt_char]
                insert_candidates_before = [(c, 1.0) for c in ['å•Š', 'å‘¢', 'å§', 'å‘€', 'å“¦'] if c not in filter_words]
            else:
                replace_candidates = _generate_candidates_clare(current_text, idx, tokenizer, mlm_model, k, max_length)
                insert_candidates_before = _generate_insert_candidates_clare(current_text, idx, tokenizer, mlm_model, k,
                                                                            max_length)

            for cand, _ in replace_candidates[:10]:
                new_text = ''.join(words[:idx]) + cand + ''.join(words[idx + 1:])
                probs = _predict_with_target_model(new_text, tgt_model, tokenizer, device, max_length)
                feature.query += 1
                new_label = int(torch.argmax(probs).item())
                if new_label != orig_label:
                    feature.change += 1
                    feature.changes.append({
                        'op': 'replace',
                        'position': idx,
                        'original': tgt_char,
                        'replacement': cand,
                        'success': True
                    })
                    feature.final_adverse = new_text
                    feature.success = 4
                    return feature
                gap = current_orig_prob - float(probs[orig_label].item())
                if gap > 0:
                    if best is None or gap > best['gap']:
                        best = {
                            'op': 'replace',
                            'position': idx,
                            'original': tgt_char,
                            'replacement': cand,
                            'gap': gap,
                            'new_text': new_text
                        }

            for cand, _ in insert_candidates_before[:10]:
                new_text = ''.join(words[:idx]) + cand + ''.join(words[idx:])
                probs = _predict_with_target_model(new_text, tgt_model, tokenizer, device, max_length)
                feature.query += 1
                new_label = int(torch.argmax(probs).item())
                if new_label != orig_label:
                    feature.change += 1
                    feature.changes.append({
                        'op': 'insert',
                        'position': idx,
                        'original': '',
                        'replacement': cand,
                        'success': True
                    })
                    feature.final_adverse = new_text
                    feature.success = 4
                    return feature
                gap = current_orig_prob - float(probs[orig_label].item())
                if gap > 0:
                    if best is None or gap > best['gap']:
                        best = {
                            'op': 'insert',
                            'position': idx,
                            'original': '',
                            'replacement': cand,
                            'gap': gap,
                            'new_text': new_text
                        }

            if idx < len(words) - 1:
                new_text = ''.join(words[:idx]) + ''.join(words[idx + 1:])
            else:
                new_text = ''.join(words[:idx])

            if new_text != current_text and new_text.strip() != '':
                probs = _predict_with_target_model(new_text, tgt_model, tokenizer, device, max_length)
                feature.query += 1
                new_label = int(torch.argmax(probs).item())
                if new_label != orig_label:
                    feature.change += 1
                    feature.changes.append({
                        'op': 'merge',
                        'position': idx,
                        'original': tgt_char,
                        'replacement': '',
                        'success': True
                    })
                    feature.final_adverse = new_text
                    feature.success = 4
                    return feature
                gap = current_orig_prob - float(probs[orig_label].item())
                if gap > 0:
                    if best is None or gap > best['gap']:
                        best = {
                            'op': 'merge',
                            'position': idx,
                            'original': tgt_char,
                            'replacement': '',
                            'gap': gap,
                            'new_text': new_text
                        }

        if best is None:
            break

        feature.change += 1
        feature.changes.append({
            'op': best['op'],
            'position': best['position'],
            'original': best['original'],
            'replacement': best['replacement'],
            'success': False,
            'gap': float(best['gap'])
        })
        current_text = best['new_text']

    feature.final_adverse = current_text
    feature.success = 2
    return feature


def evaluate_fraud_attack(features, output_json=None):
    """
    æ¬ºè¯ˆå¯¹è¯æ”»å‡»è¯„ä¼°
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¬ºè¯ˆå¯¹è¯å¯¹æŠ—æ”»å‡»è¯„ä¼°ç»“æœ")
    print("=" * 60)

    total = len(features)
    success_count = 0
    original_error = 0
    total_queries = 0
    total_changes = 0
    total_words = 0

    success_features = []

    for feat in features:
        total_words += len(feat.seq)

        if feat.success == 3:
            original_error += 1
        elif feat.success == 4:
            success_count += 1
            total_queries += feat.query
            total_changes += feat.change
            success_features.append(feat)

    # è®¡ç®—æŒ‡æ ‡
    if success_count > 0:
        avg_queries = total_queries / success_count
        avg_change_rate = total_changes / total_words if total_words > 0 else 0
    else:
        avg_queries = 0
        avg_change_rate = 0

    original_accuracy = 1 - (original_error / total) if total > 0 else 0
    attack_success_rate = success_count / (total - original_error) if (total - original_error) > 0 else 0
    after_attack_accuracy = 1 - attack_success_rate

    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"åŸå§‹é¢„æµ‹é”™è¯¯: {original_error} ({original_error / total * 100:.2f}%)")
    print(f"æ”»å‡»æˆåŠŸ: {success_count} ({success_count / total * 100:.2f}%)")
    print(f"æ”»å‡»å¤±è´¥: {total - original_error - success_count}")
    print(f"\næ”»å‡»å‰å‡†ç¡®ç‡: {original_accuracy:.4f}")
    print(f"æ”»å‡»åå‡†ç¡®ç‡: {after_attack_accuracy:.4f}")
    print(f"æ”»å‡»æˆåŠŸç‡: {attack_success_rate:.4f}")
    print(f"å¹³å‡æŸ¥è¯¢æ¬¡æ•°: {avg_queries:.2f}")
    print(f"å¹³å‡æ”¹åŠ¨ç‡: {avg_change_rate:.4f}")

    # ä¿å­˜æˆåŠŸæ¡ˆä¾‹
    if output_json and success_features:
        output_data = []
        for feat in success_features:
            output_data.append({
                'original_text': feat.seq,
                'adversarial_text': feat.final_adverse,
                'label': int(feat.label),
                'query_times': int(feat.query),
                'changes': feat.changes,
                'change_count': int(feat.change),
                'text_length': len(feat.seq)
            })

        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æˆåŠŸæ¡ˆä¾‹å·²ä¿å­˜åˆ°: {output_json}")

    return {
        'total_samples': total,
        'original_accuracy': original_accuracy,
        'after_attack_accuracy': after_attack_accuracy,
        'attack_success_rate': attack_success_rate,
        'avg_queries': avg_queries,
        'avg_change_rate': avg_change_rate,
        'success_count': success_count
    }


def main():
    """
    ä¸»å‡½æ•° - æ”¯æŒä¸¤ç§æ¨¡å‹ç‰ˆæœ¬
    """
    print("=" * 60)
    print("ğŸ”§ æ¬ºè¯ˆå¯¹è¯å¯¹æŠ—æ”»å‡»ç³»ç»Ÿ (æ”¯æŒBERTå’Œåˆ†ç±»å™¨æ¨¡å‹)")
    print("=" * 60)

    # ========== ç¡¬ç¼–ç å‚æ•°é…ç½® ==========
    DATA_PATH = "data/processed/fraud_test_small.txt"  # å°æ ·æœ¬æµ‹è¯•
    MODEL_TYPE = "base"  # "bert" æˆ– "base"

    if MODEL_TYPE == "bert":
        TGT_PATH = "models/bert_fraud_classifier/best_model.pt"  # BERTæ¨¡å‹è·¯å¾„
        MLM_PATH = "bert-base-chinese"  # MLMæ¨¡å‹
    else:
        TGT_PATH = "models/classifier/best_model.pt"  # åˆ†ç±»å™¨æ¨¡å‹è·¯å¾„
        MLM_PATH = None  # åˆ†ç±»å™¨æ¨¡å‹ä¸éœ€è¦MLM

    OUTPUT_DIR = "./results"
    K = 20
    BATCH_SIZE = 32
    MAX_LENGTH = 128
    THRESHOLD_PRED_SCORE = 0.3
    MAX_CHANGE_RATE = 0.4
    START_IDX = 0
    END_IDX = 50

    # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âš¡ ä½¿ç”¨è®¾å¤‡: {device}")

    # ========== ä¸»ç¨‹åºå¼€å§‹ ==========

    print(f"ğŸ“‚ æ•°æ®è·¯å¾„: {DATA_PATH}")
    print(f"ğŸ¯ ç›®æ ‡æ¨¡å‹: {TGT_PATH}")
    print(f"ğŸ¤– æ¨¡å‹ç±»å‹: {MODEL_TYPE}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(DATA_PATH):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {DATA_PATH}")
        return

    if not os.path.exists(TGT_PATH):
        print(f"âŒ ç›®æ ‡æ¨¡å‹ä¸å­˜åœ¨: {TGT_PATH}")
        if MODEL_TYPE == "bert":
            print("è¯·å…ˆè¿è¡Œ train_bert.py è®­ç»ƒBERTæ¨¡å‹")
        else:
            print("è¯·å…ˆè¿è¡Œ train_classifiers.py è®­ç»ƒåˆ†ç±»å™¨æ¨¡å‹")
        return

    # åŠ è½½æ¨¡å‹
    print("\nâ³ åŠ è½½æ¨¡å‹ä¸­...")

    # åŠ è½½ç›®æ ‡æ¨¡å‹
    try:
        checkpoint = torch.load(TGT_PATH, map_location=device)

        if MODEL_TYPE == "bert":
            # BERTæ¨¡å‹
            from transformers import AutoTokenizer as BertTokenizer
            tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')

            # åˆ›å»ºæ¨¡å‹ç»“æ„
            model = BertClassifier('bert-base-chinese', 2)
            model.load_state_dict(checkpoint['model_state_dict'])

            # åŠ è½½MLMæ¨¡å‹
            from transformers import BertForMaskedLM
            mlm_model = BertForMaskedLM.from_pretrained(MLM_PATH)
            mlm_model.to(device)
            mlm_model.eval()
            print(f"âœ… MLMæ¨¡å‹åŠ è½½æˆåŠŸ")

        else:
            # åˆ†ç±»å™¨æ¨¡å‹
            # åŠ è½½tokenizer
            tokenizer_path = os.path.join(os.path.dirname(TGT_PATH), 'tokenizer.pkl')
            if os.path.exists(tokenizer_path):
                with open(tokenizer_path, 'rb') as f:
                    tokenizer = pickle.load(f)
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°tokenizeræ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                tokenizer = Tokenizer()

            # åˆ›å»ºæ¨¡å‹ç»“æ„
            vocab_size = checkpoint.get('vocab_size', 5000)
            embedding_dim = checkpoint.get('embedding_dim', 128)
            hidden_dim = checkpoint.get('hidden_dim', 256)

            model = Classifier(
                vocab_size=vocab_size,
                embedding_dim=embedding_dim,
                hidden_dim=hidden_dim,
                num_classes=2
            )
            model.load_state_dict(checkpoint['model_state_dict'])

            # åˆ†ç±»å™¨æ¨¡å‹ä¸éœ€è¦MLM
            mlm_model = None

        model.to(device)
        model.eval()

        print(f"âœ… ç›®æ ‡æ¨¡å‹åŠ è½½æˆåŠŸ (éªŒè¯å‡†ç¡®ç‡: {checkpoint.get('val_acc', 'æœªçŸ¥'):.4f})")

    except Exception as e:
        print(f"âŒ ç›®æ ‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # åŠ è½½æ•°æ®
    print("\nâ³ åŠ è½½æ•°æ®ä¸­...")
    features_raw = get_fraud_data_cls(DATA_PATH)

    if not features_raw:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return

    if END_IDX is None or END_IDX > len(features_raw):
        END_IDX = len(features_raw)

    features_to_attack = features_raw[START_IDX:END_IDX]
    print(f"ğŸ¯ æ”»å‡»èŒƒå›´: {START_IDX} åˆ° {END_IDX} (å…± {len(features_to_attack)} æ¡)")

    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    labels = [label for _, label in features_to_attack]
    fraud_count = sum(labels)
    normal_count = len(labels) - fraud_count
    print(f"ğŸ“Š æ ·æœ¬åˆ†å¸ƒ: æ¬ºè¯ˆ {fraud_count} æ¡, æ­£å¸¸ {normal_count} æ¡")

    # æ‰§è¡Œæ”»å‡»
    print(f"\nâš¡ å¼€å§‹å¯¹æŠ—æ”»å‡»...")
    attacked_features = []

    start_time = time.time()

    with torch.no_grad():
        for i, (seq, label) in enumerate(tqdm(features_to_attack, desc="æ”»å‡»è¿›åº¦", unit="æ¡")):
            try:
                feature = FraudDialogueFeature(seq, label)
                feature = attack_fraud_dialogue(
                    feature,
                    model,  # ç›®æ ‡æ¨¡å‹
                    mlm_model,
                    tokenizer,
                    K,
                    BATCH_SIZE,
                    device,  # ä¼ å…¥è®¾å¤‡å‚æ•°
                    max_length=MAX_LENGTH,
                    use_bpe=0,
                    threshold_pred_score=THRESHOLD_PRED_SCORE,
                    max_change_rate=MAX_CHANGE_RATE
                )
                attacked_features.append(feature)

            except Exception as e:
                print(f"âš ï¸  ç¬¬ {i + 1} æ¡æ ·æœ¬æ”»å‡»å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                failed_feature = FraudDialogueFeature(seq, label)
                failed_feature.success = 0
                failed_feature.final_adverse = seq
                attacked_features.append(failed_feature)

    end_time = time.time()
    total_time = end_time - start_time

    print(f"\nâœ… æ”»å‡»å®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’")

    # è¯„ä¼°ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_json = os.path.join(OUTPUT_DIR, f"attack_results_{MODEL_TYPE}_{timestamp}.json")
    stats_json = os.path.join(OUTPUT_DIR, f"attack_stats_{MODEL_TYPE}_{timestamp}.json")

    print(f"\nğŸ“ˆ è¯„ä¼°æ”»å‡»æ•ˆæœ...")
    stats = evaluate_fraud_attack(attacked_features, output_json)

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    with open(stats_json, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ç»“æœæ–‡ä»¶:")
    print(f"  è¯¦ç»†ç»“æœ: {output_json}")
    print(f"  ç»Ÿè®¡ä¿¡æ¯: {stats_json}")
    print("=" * 60)
    print(f"ğŸ‰ {MODEL_TYPE.upper()}æ¨¡å‹å¯¹æŠ—æ”»å‡»å®éªŒå®Œæˆï¼")
    print("=" * 60)


if __name__ == '__main__':
    main()
