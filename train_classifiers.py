# -*- coding: utf-8 -*-
"""
æ–‡æœ¬åˆ†ç±»å™¨
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import os
import re
from collections import Counter
import pickle


class Tokenizer:

    def __init__(self, vocab_size=5000):
        self.vocab_size = vocab_size
        self.char_to_idx = {}
        self.idx_to_char = {}
        self.unk_token = '[UNK]'
        self.pad_token = '[PAD]'
        self.cls_token = '[CLS]'
        self.sep_token = '[SEP]'

    def build_vocab(self, texts):
        """æ„å»ºè¯æ±‡è¡¨"""
        # ç»Ÿè®¡æ‰€æœ‰å­—ç¬¦
        char_counter = Counter()
        for text in texts:
            char_counter.update(text)

        # é€‰æ‹©æœ€å¸¸è§çš„å­—ç¬¦
        most_common = char_counter.most_common(self.vocab_size - 4)  # ä¸ºç‰¹æ®Štokenç•™ä½ç½®

        # æ„å»ºæ˜ å°„
        self.char_to_idx = {self.unk_token: 0, self.pad_token: 1, self.cls_token: 2, self.sep_token: 3}
        self.idx_to_char = {0: self.unk_token, 1: self.pad_token, 2: self.cls_token, 3: self.sep_token}

        for idx, (char, _) in enumerate(most_common, start=4):
            self.char_to_idx[char] = idx
            self.idx_to_char[idx] = char

    def encode(self, text, max_length=128):
        """ç¼–ç æ–‡æœ¬"""
        tokens = [self.char_to_idx.get(char, self.char_to_idx[self.unk_token]) for char in text[:max_length - 2]]

        # æ·»åŠ ç‰¹æ®Štoken
        tokens = [self.char_to_idx[self.cls_token]] + tokens + [self.char_to_idx[self.sep_token]]

        # å¡«å……
        if len(tokens) < max_length:
            tokens = tokens + [self.char_to_idx[self.pad_token]] * (max_length - len(tokens))
        else:
            tokens = tokens[:max_length]
            tokens[-1] = self.char_to_idx[self.sep_token]

        # åˆ›å»ºattention mask
        attention_mask = [1 if token != self.char_to_idx[self.pad_token] else 0 for token in tokens]

        return {
            'input_ids': torch.tensor(tokens, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)
        }


class Classifier(nn.Module):
    """åˆ†ç±»å™¨"""

    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_classes=2):
        super(Classifier, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, input_ids, attention_mask=None):
        # åµŒå…¥å±‚
        embedded = self.embedding(input_ids)

        # LSTM
        lstm_out, _ = self.lstm(embedded)

        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_hidden = lstm_out[:, -1, :]

        # åˆ†ç±»
        output = self.dropout(last_hidden)
        logits = self.fc(output)

        return logits


class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoded = self.tokenizer.encode(text, self.max_length)

        return {
            'input_ids': encoded['input_ids'],
            'attention_mask': encoded['attention_mask'],
            'label': torch.tensor(label, dtype=torch.long)
        }


def load_data(data_path):
    """åŠ è½½æ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")

    texts = []
    labels = []

    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # è·³è¿‡è¡¨å¤´
        if lines[0].strip() == 'text_a\tlabel':
            lines = lines[1:]

        for line in lines:
            line = line.strip()
            if not line:
                continue
            parts = line.split('\t')
            if len(parts) >= 2:
                text = parts[0].strip()
                if text:
                    texts.append(text)
                    try:
                        labels.append(int(parts[1]))
                    except:
                        labels.append(1)

        print(f"âœ… åŠ è½½äº† {len(texts)} æ¡æ ·æœ¬")

    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")

    return texts, labels


def train_model():
    """è®­ç»ƒæ¨¡å‹"""
    print("=" * 60)
    print("ğŸ¤– æ–‡æœ¬åˆ†ç±»å™¨è®­ç»ƒ")
    print("=" * 60)

    # é…ç½®å‚æ•°
    TRAIN_DATA = "data/processed/fraud_train.txt"
    VAL_DATA = "data/processed/fraud_val.txt"
    OUTPUT_DIR = "models/classifier"

    VOCAB_SIZE = 5000
    EMBEDDING_DIM = 128
    HIDDEN_DIM = 256
    MAX_LENGTH = 128
    BATCH_SIZE = 32
    LEARNING_RATE = 0.001
    EPOCHS = 10

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    train_texts, train_labels = load_data(TRAIN_DATA)

    if os.path.exists(VAL_DATA):
        val_texts, val_labels = load_data(VAL_DATA)
    else:
        # åˆ†å‰²éªŒè¯é›†
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels
        )

    # é™åˆ¶æ ·æœ¬æ•°é‡ç”¨äºå¿«é€Ÿè®­ç»ƒ
    SAMPLE_LIMIT = 1000
    if len(train_texts) > SAMPLE_LIMIT:
        print(f"ğŸ”§ é‡‡æ · {SAMPLE_LIMIT} æ¡æ ·æœ¬è¿›è¡Œè®­ç»ƒ...")
        train_texts = train_texts[:SAMPLE_LIMIT]
        train_labels = train_labels[:SAMPLE_LIMIT]

    if len(val_texts) > 200:
        val_texts = val_texts[:200]
        val_labels = val_labels[:200]

    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†: {len(train_texts)} æ¡")
    print(f"    æ¬ºè¯ˆ: {sum(train_labels)} æ¡ ({sum(train_labels) / len(train_labels) * 100:.1f}%)")
    print(
        f"    æ­£å¸¸: {len(train_labels) - sum(train_labels)} æ¡ ({(len(train_labels) - sum(train_labels)) / len(train_labels) * 100:.1f}%)")

    print(f"\n  éªŒè¯é›†: {len(val_texts)} æ¡")
    print(f"    æ¬ºè¯ˆ: {sum(val_labels)} æ¡ ({sum(val_labels) / len(val_labels) * 100:.1f}%)")
    print(
        f"    æ­£å¸¸: {len(val_labels) - sum(val_labels)} æ¡ ({(len(val_labels) - sum(val_labels)) / len(val_labels) * 100:.1f}%)")

    # åˆ›å»ºtokenizerå¹¶æ„å»ºè¯æ±‡è¡¨
    print("\nğŸ”§ æ„å»ºè¯æ±‡è¡¨...")
    tokenizer = Tokenizer(VOCAB_SIZE)
    tokenizer.build_vocab(train_texts)
    print(f"âœ… è¯æ±‡è¡¨å¤§å°: {len(tokenizer.char_to_idx)}")

    # ä¿å­˜tokenizer
    with open(os.path.join(OUTPUT_DIR, 'tokenizer.pkl'), 'wb') as f:
        pickle.dump(tokenizer, f)

    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ“¦ åˆ›å»ºæ•°æ®é›†...")
    train_dataset = TextDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    print(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    print(f"  éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")

    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    model = Classifier(
        vocab_size=VOCAB_SIZE,
        embedding_dim=EMBEDDING_DIM,
        hidden_dim=HIDDEN_DIM,
        num_classes=2
    )

    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    print(f"âš¡ ä½¿ç”¨è®¾å¤‡: {device}")

    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    # è®­ç»ƒå¾ªç¯
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_val_acc = 0

    for epoch in range(EPOCHS):
        print(f"\nğŸ“… Epoch {epoch + 1}/{EPOCHS}")

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)

            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()

        avg_train_loss = train_loss / len(train_loader)
        train_acc = train_correct / train_total if train_total > 0 else 0

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_preds = []
        val_true = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label'].to(device)

                outputs = model(input_ids, attention_mask)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                val_preds.extend(predicted.cpu().numpy())
                val_true.extend(labels.cpu().numpy())

        avg_val_loss = val_loss / len(val_loader)
        val_acc = accuracy_score(val_true, val_preds)

        print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
        print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'vocab_size': VOCAB_SIZE,
                'embedding_dim': EMBEDDING_DIM,
                'hidden_dim': HIDDEN_DIM,
            }, os.path.join(OUTPUT_DIR, 'best_model.pt'))
            print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {val_acc:.4f})")

    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ¯ æœ€ç»ˆè¯„ä¼°...")

    # åŠ è½½æœ€ä½³æ¨¡å‹
    checkpoint = torch.load(os.path.join(OUTPUT_DIR, 'best_model.pt'))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    all_preds = []
    all_true = []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids, attention_mask)
            _, predicted = torch.max(outputs.data, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_true.extend(labels.cpu().numpy())

    final_acc = accuracy_score(all_true, all_preds)
    final_f1 = f1_score(all_true, all_preds, average='weighted')

    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"  éªŒè¯å‡†ç¡®ç‡: {final_acc:.4f}")
    print(f"  F1åˆ†æ•°: {final_f1:.4f}")

    print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_true, all_preds, target_names=['æ­£å¸¸', 'æ¬ºè¯ˆ'], digits=4))

    print("\nğŸ”¢ æ··æ·†çŸ©é˜µ:")
    cm = confusion_matrix(all_true, all_preds)
    print(f"[[TN FP]\n [FN TP]] = \n{cm}")

    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)

    print(f"\nğŸ“ æ¨¡å‹ä¿å­˜åˆ°: {OUTPUT_DIR}")
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹: {OUTPUT_DIR}/best_model.pt")
    print(f"ğŸ’¾ Tokenizer: {OUTPUT_DIR}/tokenizer.pkl")

    return model, tokenizer


if __name__ == "__main__":
    train_model()