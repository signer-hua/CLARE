# -*- coding: utf-8 -*-
"""
æ‰¹é‡è¿è¡Œå¯¹æŠ—æ”»å‡»å®éªŒ - ç›´æ¥è¿è¡Œç‰ˆæœ¬
"""

import os
import sys
import json
from datetime import datetime
import pandas as pd
import torch
import time
import pickle
from tqdm import tqdm
import copy
import re

# å°†å½“å‰ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from bertattack_fraud import (
        FraudDialogueFeature,
        Classifier,
        Tokenizer,
        clean_dialogue_for_attack,
        _tokenize_chinese_dialogue,
        get_important_scores_fraud,
        attack_fraud_dialogue,
        evaluate_fraud_attack
    )
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥æ¨¡å—: {e}")
    print("è¯·ç¡®ä¿ bertattack_fraud.py åœ¨åŒä¸€ç›®å½•ä¸‹")
    sys.exit(1)


def get_fraud_data_cls(data_path):
    """
    åŠ è½½æ¬ºè¯ˆå¯¹è¯æ•°æ®
    """
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")

    texts = []
    labels = []

    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # è·³è¿‡è¡¨å¤´
        if lines[0].strip() == 'text_a\tlabel':
            lines = lines[1:]

        for line in lines:
            line = line.strip()
            if not line:
                continue
            parts = line.split('\t')
            if len(parts) >= 2:
                text = clean_dialogue_for_attack(parts[0])
                if text:
                    texts.append(text)
                    try:
                        labels.append(int(parts[1]))
                    except:
                        labels.append(1)

        print(f"âœ… åŠ è½½äº† {len(texts)} æ¡æ ·æœ¬")

    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")

    return list(zip(texts, labels))


def run_single_attack_experiment(data_path, model_path, model_type, output_dir,
                                 k=20, batch_size=32, max_length=128,
                                 threshold_pred_score=0.3, max_change_rate=0.4,
                                 start_idx=0, end_idx=50, device=None):
    """
    è¿è¡Œå•ä¸ªæ”»å‡»å®éªŒ
    """
    print(f"\nğŸ”§ å‡†å¤‡æ”»å‡»å®éªŒ: {model_path}")

    # åˆ›å»ºå®éªŒè¾“å‡ºç›®å½•
    model_name = os.path.basename(model_path).replace('.pt', '').replace('.pth', '')
    exp_dir = os.path.join(output_dir, f"attack_{model_type}_{model_name}")
    os.makedirs(exp_dir, exist_ok=True)

    # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    print(f"âš¡ ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½æ¨¡å‹
    try:
        checkpoint = torch.load(model_path, map_location=device)

        if model_type == "bert":
            # å¯¼å…¥BERTç›¸å…³æ¨¡å—
            from transformers import AutoTokenizer, AutoModelForMaskedLM
            from bertattack_fraud import BertClassifier

            tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')

            # åˆ›å»ºæ¨¡å‹ç»“æ„
            model = BertClassifier('bert-base-chinese', 2)
            model.load_state_dict(checkpoint['model_state_dict'])

            # åŠ è½½MLMæ¨¡å‹
            mlm_model = AutoModelForMaskedLM.from_pretrained('bert-base-chinese')
            mlm_model.to(device)
            mlm_model.eval()

        else:  # base model
            # åŠ è½½tokenizer
            tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer.pkl')
            if os.path.exists(tokenizer_path):
                with open(tokenizer_path, 'rb') as f:
                    tokenizer = pickle.load(f)
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°tokenizeræ–‡ä»¶ï¼Œåˆ›å»ºé»˜è®¤tokenizer")
                tokenizer = Tokenizer()

            # åˆ›å»ºæ¨¡å‹ç»“æ„
            vocab_size = checkpoint.get('vocab_size', 5000)
            embedding_dim = checkpoint.get('embedding_dim', 128)
            hidden_dim = checkpoint.get('hidden_dim', 256)

            model = Classifier(
                vocab_size=vocab_size,
                embedding_dim=embedding_dim,
                hidden_dim=hidden_dim,
                num_classes=2
            )
            model.load_state_dict(checkpoint['model_state_dict'])
            mlm_model = None

        model.to(device)
        model.eval()

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (éªŒè¯å‡†ç¡®ç‡: {checkpoint.get('val_acc', 'æœªçŸ¥'):.4f})")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # åŠ è½½æ•°æ®
    features_raw = get_fraud_data_cls(data_path)
    if not features_raw:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return None

    if end_idx is None or end_idx > len(features_raw):
        end_idx = len(features_raw)

    features_to_attack = features_raw[start_idx:end_idx]
    print(f"ğŸ¯ æ”»å‡»èŒƒå›´: {start_idx} åˆ° {end_idx} (å…± {len(features_to_attack)} æ¡)")

    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    labels = [label for _, label in features_to_attack]
    fraud_count = sum(labels)
    normal_count = len(labels) - fraud_count
    print(f"ğŸ“Š æ ·æœ¬åˆ†å¸ƒ: æ¬ºè¯ˆ {fraud_count} æ¡, æ­£å¸¸ {normal_count} æ¡")

    # æ‰§è¡Œæ”»å‡»
    print(f"âš¡ å¼€å§‹å¯¹æŠ—æ”»å‡»...")
    attacked_features = []

    start_time = time.time()

    with torch.no_grad():
        for i, (seq, label) in enumerate(tqdm(features_to_attack, desc="æ”»å‡»è¿›åº¦", unit="æ¡")):
            try:
                feature = FraudDialogueFeature(seq, label)
                feature = attack_fraud_dialogue(
                    feature,
                    model,
                    mlm_model,
                    tokenizer,
                    k,
                    batch_size,
                    device,
                    max_length=max_length,
                    use_bpe=0,
                    threshold_pred_score=threshold_pred_score,
                    max_change_rate=max_change_rate
                )
                attacked_features.append(feature)

            except Exception as e:
                print(f"âš ï¸  ç¬¬ {i + 1} æ¡æ ·æœ¬æ”»å‡»å¤±è´¥: {e}")
                failed_feature = FraudDialogueFeature(seq, label)
                failed_feature.success = 0
                failed_feature.final_adverse = seq
                attacked_features.append(failed_feature)

    end_time = time.time()
    total_time = end_time - start_time

    print(f"âœ… æ”»å‡»å®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’")

    # è¯„ä¼°ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_json = os.path.join(exp_dir, f"attack_results_{timestamp}.json")
    stats_json = os.path.join(exp_dir, f"attack_stats_{timestamp}.json")

    print(f"ğŸ“ˆ è¯„ä¼°æ”»å‡»æ•ˆæœ...")
    stats = evaluate_fraud_attack(attacked_features, output_json)

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    with open(stats_json, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    # ä¿å­˜å®éªŒæ—¥å¿—
    log_file = os.path.join(exp_dir, "experiment_log.txt")
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"å®éªŒæ—¶é—´: {timestamp}\n")
        f.write(f"æ¨¡å‹è·¯å¾„: {model_path}\n")
        f.write(f"æ¨¡å‹ç±»å‹: {model_type}\n")
        f.write(f"æ•°æ®è·¯å¾„: {data_path}\n")
        f.write(f"æ ·æœ¬æ•°é‡: {len(features_to_attack)}\n")
        f.write(f"æ”»å‡»å‚æ•°: k={k}, batch_size={batch_size}, max_length={max_length}\n")
        f.write(f"æ€»è€—æ—¶: {total_time:.2f}ç§’\n\n")
        f.write(f"æ”»å‡»æˆåŠŸç‡: {stats.get('attack_success_rate', 0) * 100:.2f}%\n")
        f.write(f"åŸå§‹å‡†ç¡®ç‡: {stats.get('original_accuracy', 0) * 100:.2f}%\n")
        f.write(f"æ”»å‡»åå‡†ç¡®ç‡: {stats.get('after_attack_accuracy', 0) * 100:.2f}%\n")

    print(f"ğŸ’¾ å®éªŒæ–‡ä»¶:")
    print(f"  è¯¦ç»†ç»“æœ: {output_json}")
    print(f"  ç»Ÿè®¡ä¿¡æ¯: {stats_json}")
    print(f"  å®éªŒæ—¥å¿—: {log_file}")

    return {
        'success': True,
        'model_type': model_type,
        'model_path': model_path,
        'output_dir': exp_dir,
        'result_file': output_json,
        'stats_file': stats_json,
        'log_file': log_file,
        'stats': stats
    }


def find_models(model_dir):
    """
    æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
    """
    models = []

    # 1. æŸ¥æ‰¾BERTæ¨¡å‹
    bert_model_path = os.path.join(model_dir, "bert_fraud_classifier", "best_model.pt")
    if os.path.exists(bert_model_path):
        print(f"âœ… æ‰¾åˆ°BERTæ¨¡å‹: {bert_model_path}")
        models.append(("bert", bert_model_path))
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°BERTæ¨¡å‹: {bert_model_path}")

    # 2. æŸ¥æ‰¾åˆ†ç±»å™¨æ¨¡å‹
    base_model_path = os.path.join(model_dir, "classifier", "best_model.pt")
    if os.path.exists(base_model_path):
        print(f"âœ… æ‰¾åˆ°åˆ†ç±»å™¨æ¨¡å‹: {base_model_path}")
        models.append(("base", base_model_path))
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°åˆ†ç±»å™¨æ¨¡å‹: {base_model_path}")

    # 3. æŸ¥æ‰¾å…¶ä»–å¯èƒ½å­˜åœ¨çš„æ¨¡å‹
    model_patterns = ["*.pt", "*.pth", "*.ckpt"]
    for root, dirs, files in os.walk(model_dir):
        for file in files:
            if file.endswith(('.pt', '.pth', '.ckpt')):
                model_path = os.path.join(root, file)
                if model_path not in [m[1] for m in models]:
                    # å°è¯•åˆ¤æ–­æ¨¡å‹ç±»å‹
                    if 'bert' in file.lower() or 'bert' in root.lower():
                        models.append(("bert", model_path))
                        print(f"âœ… æ‰¾åˆ°å…¶ä»–BERTæ¨¡å‹: {model_path}")
                    else:
                        models.append(("base", model_path))
                        print(f"âœ… æ‰¾åˆ°å…¶ä»–åˆ†ç±»å™¨æ¨¡å‹: {model_path}")

    return models


def collect_results(results, output_dir):
    """
    æ”¶é›†æ‰€æœ‰å®éªŒç»“æœ
    """
    print("\nğŸ“Š æ”¶é›†å®éªŒç»“æœ...")

    summary = {
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'experiments': [],
        'summary_stats': {}
    }

    successful_experiments = []

    for result in results:
        if result and result.get('success'):
            exp_info = {
                'model_type': result['model_type'],
                'model_path': result['model_path'],
                'model_name': os.path.basename(result['model_path']),
                'output_dir': result['output_dir'],
                'result_file': result['result_file'],
                'stats': result.get('stats', {})
            }

            summary['experiments'].append(exp_info)
            successful_experiments.append(exp_info)

            stats = result.get('stats', {})
            success_rate = stats.get('attack_success_rate', 0) * 100
            print(f"  âœ… {result['model_type']} - {os.path.basename(result['model_path'])}: "
                  f"æˆåŠŸç‡ {success_rate:.1f}%")
        else:
            model_path = result.get('model_path', 'æœªçŸ¥') if result else 'æœªçŸ¥'
            model_type = result.get('model_type', 'æœªçŸ¥') if result else 'æœªçŸ¥'
            print(f"  âŒ æ”»å‡»å¤±è´¥ {model_type} - {os.path.basename(model_path)}")

    # æ±‡æ€»ç»Ÿè®¡
    if successful_experiments:
        models = [f"{exp['model_type']} - {exp['model_name']}" for exp in successful_experiments]
        success_rates = [exp['stats'].get('attack_success_rate', 0) for exp in successful_experiments]
        after_accuracies = [exp['stats'].get('after_attack_accuracy', 0) for exp in successful_experiments]
        original_accuracies = [exp['stats'].get('original_accuracy', 0) for exp in successful_experiments]

        summary['summary_stats'] = {
            'total_experiments': len(results),
            'successful_experiments': len(successful_experiments),
            'models': models,
            'success_rates': success_rates,
            'after_attack_accuracies': after_accuracies,
            'original_accuracies': original_accuracies,
            'avg_success_rate': sum(success_rates) / len(success_rates) if success_rates else 0,
            'avg_after_accuracy': sum(after_accuracies) / len(after_accuracies) if after_accuracies else 0,
            'avg_original_accuracy': sum(original_accuracies) / len(original_accuracies) if original_accuracies else 0
        }

    # ä¿å­˜æ±‡æ€»
    summary_file = os.path.join(output_dir, f"experiment_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    # åˆ›å»ºCSVæŠ¥å‘Š
    if successful_experiments:
        csv_data = []
        for exp in successful_experiments:
            stats = exp['stats']
            csv_data.append({
                'Model Type': exp['model_type'].upper(),
                'Model': exp['model_name'],
                'Attack Success Rate': f"{stats.get('attack_success_rate', 0) * 100:.2f}%",
                'After Attack Accuracy': f"{stats.get('after_attack_accuracy', 0) * 100:.2f}%",
                'Original Accuracy': f"{stats.get('original_accuracy', 0) * 100:.2f}%",
                'Avg Queries': f"{stats.get('avg_queries', 0):.1f}",
                'Avg Change Rate': f"{stats.get('avg_change_rate', 0) * 100:.2f}%",
                'Success Count': stats.get('success_count', 0),
                'Total Samples': stats.get('total_samples', 0)
            })

        df = pd.DataFrame(csv_data)
        csv_file = os.path.join(output_dir, f"results_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')

        print(f"\nğŸ’¾ CSVæŠ¥å‘Šå·²ä¿å­˜åˆ°: {csv_file}")
        print("\nğŸ“ˆ æ”»å‡»ç»“æœæ±‡æ€»:")
        print(df.to_string(index=False))

    return summary


def main():
    """
    æ‰¹é‡æ”»å‡»å®éªŒä¸»å‡½æ•°
    """
    print("=" * 60)
    print("æ¬ºè¯ˆå¯¹è¯å¯¹æŠ—æ”»å‡»æ‰¹é‡å®éªŒç³»ç»Ÿ")
    print("=" * 60)

    # ========== ç¡¬ç¼–ç å‚æ•°é…ç½® ==========
    # åœ¨è¿™é‡Œä¿®æ”¹å‚æ•°ï¼Œç„¶åç›´æ¥è¿è¡Œ

    # æ•°æ®è·¯å¾„
    DATA_PATH = "data/processed/fraud_test_small.txt"  # å°æ ·æœ¬æµ‹è¯•
    # DATA_PATH = "data/processed/fraud_test.txt"      # å®Œæ•´æµ‹è¯•é›†

    # æ¨¡å‹ç›®å½•
    MODEL_DIR = "models"

    # è¾“å‡ºç›®å½•
    OUTPUT_DIR = "./experiments"

    # å®éªŒé…ç½®
    SAMPLE_SIZE = 50  # æ¯ä¸ªå®éªŒçš„æ ·æœ¬æ•°
    START_IDX = 0  # èµ·å§‹ç´¢å¼•

    # æ”»å‡»å‚æ•°
    K = 20  # Top-Kå€™é€‰è¯
    BATCH_SIZE = 32  # æ‰¹é‡å¤§å°
    MAX_LENGTH = 128  # æœ€å¤§æ–‡æœ¬é•¿åº¦
    THRESHOLD_PRED_SCORE = 0.3
    MAX_CHANGE_RATE = 0.4

    # ========== ä¸»ç¨‹åºå¼€å§‹ ==========

    print("\nğŸ“‹ é…ç½®å‚æ•°:")
    print(f"  æ•°æ®è·¯å¾„: {DATA_PATH}")
    print(f"  æ¨¡å‹ç›®å½•: {MODEL_DIR}")
    print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"  æ ·æœ¬æ•°é‡: {SAMPLE_SIZE}")
    print(f"  Top-K: {K}, æ‰¹é‡å¤§å°: {BATCH_SIZE}")

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(DATA_PATH):
        print(f"\nâŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {DATA_PATH}")
        print("è¯·å…ˆè¿è¡Œ preprocess_fraud.py é¢„å¤„ç†æ•°æ®")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_dir = os.path.join(OUTPUT_DIR, f"batch_experiment_{timestamp}")
    os.makedirs(exp_dir, exist_ok=True)

    print(f"\nğŸ“ å®éªŒç›®å½•: {exp_dir}")

    # æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨æ¨¡å‹
    print("\nğŸ” æŸ¥æ‰¾å¯ç”¨æ¨¡å‹...")
    models = find_models(MODEL_DIR)

    if not models:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ¨¡å‹ï¼")
        print("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤è®­ç»ƒæ¨¡å‹:")
        print("1. è®­ç»ƒåˆ†ç±»å™¨æ¨¡å‹: python train_classifiers.py")
        print("2. è®­ç»ƒBERTæ¨¡å‹: python train_bert.py (å¦‚æœéœ€è¦)")
        return

    print(f"\nâœ… æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹")

    # è¿è¡Œæ‰€æœ‰æ”»å‡»å®éªŒ
    print(f"\nâš¡ å¼€å§‹æ‰¹é‡æ”»å‡»å®éªŒ (å…± {len(models)} ä¸ªæ¨¡å‹)")
    results = []

    for i, (model_type, model_path) in enumerate(models):
        print(f"\n{'=' * 50}")
        print(f"å®éªŒ {i + 1}/{len(models)}: {model_type.upper()} - {os.path.basename(model_path)}")
        print(f"{'=' * 50}")

        # è®¡ç®—æ ·æœ¬èŒƒå›´
        start = START_IDX
        end = start + SAMPLE_SIZE

        result = run_single_attack_experiment(
            data_path=DATA_PATH,
            model_path=model_path,
            model_type=model_type,
            output_dir=exp_dir,
            k=K,
            batch_size=BATCH_SIZE,
            max_length=MAX_LENGTH,
            threshold_pred_score=THRESHOLD_PRED_SCORE,
            max_change_rate=MAX_CHANGE_RATE,
            start_idx=start,
            end_idx=end
        )

        results.append(result)

    # æ”¶é›†ç»“æœ
    print(f"\n{'=' * 50}")
    print("ğŸ“Š æ”¶é›†å®éªŒç»“æœ...")
    print(f"{'=' * 50}")

    summary = collect_results(results, exp_dir)

    print(f"\n{'=' * 50}")
    print("ğŸ‰ æ‰¹é‡å®éªŒå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœç›®å½•: {exp_dir}")
    print(f"{'=' * 50}")

    # æ˜¾ç¤ºå…³é”®ç»“æœ
    if summary.get('summary_stats'):
        stats = summary['summary_stats']
        print(f"\nğŸ“ˆ å…³é”®ç»Ÿè®¡:")
        print(f"  å¹³å‡æ”»å‡»æˆåŠŸç‡: {stats['avg_success_rate'] * 100:.1f}%")
        print(f"  å¹³å‡æ”»å‡»åå‡†ç¡®ç‡: {stats['avg_after_accuracy'] * 100:.1f}%")
        print(f"  å¹³å‡åŸå§‹å‡†ç¡®ç‡: {stats['avg_original_accuracy'] * 100:.1f}%")
        print(f"  æˆåŠŸå®éªŒæ•°: {stats['successful_experiments']}/{stats['total_experiments']}")

    print("\nğŸ” æŸ¥çœ‹è¯¦ç»†ç»“æœ:")
    print(f"  1. æ‰“å¼€ç›®å½•: {exp_dir}")
    print(f"  2. æŸ¥çœ‹CSVæŠ¥å‘Šæ–‡ä»¶")
    print(f"  3. æŸ¥çœ‹å„å®éªŒçš„è¯¦ç»†æ—¥å¿—")

    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥:")
    print(f"  1. åˆ†æå¯¹æŠ—æ ·æœ¬ä»¥äº†è§£æ”»å‡»æ–¹å¼")
    print(f"  2. è°ƒæ•´æ”»å‡»å‚æ•°é‡æ–°å®éªŒ")
    print(f"  3. è®­ç»ƒæ›´å¤šæ¨¡å‹è¿›è¡Œå¯¹æ¯”")


if __name__ == "__main__":
    main()